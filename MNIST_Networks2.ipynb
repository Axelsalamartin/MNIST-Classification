{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 0  accuracy :  0.4  loss :  1.86015\n",
      "epoch 100  accuracy :  0.93  loss :  0.294757\n",
      "epoch 200  accuracy :  0.95  loss :  0.29681\n",
      "epoch 300  accuracy :  0.95  loss :  0.240351\n",
      "epoch 400  accuracy :  0.89  loss :  0.478298\n",
      "epoch 500  accuracy :  0.92  loss :  0.274017\n",
      "epoch 600  accuracy :  0.92  loss :  0.3492\n",
      "epoch 700  accuracy :  0.93  loss :  0.27955\n",
      "epoch 800  accuracy :  0.93  loss :  0.209198\n",
      "epoch 900  accuracy :  0.9  loss :  0.387948\n",
      "epoch 1000  accuracy :  0.94  loss :  0.238272\n",
      "epoch 1100  accuracy :  0.91  loss :  0.279545\n",
      "epoch 1200  accuracy :  0.94  loss :  0.186909\n",
      "epoch 1300  accuracy :  0.96  loss :  0.20465\n",
      "epoch 1400  accuracy :  0.92  loss :  0.290788\n",
      "epoch 1500  accuracy :  0.91  loss :  0.303719\n",
      "epoch 1600  accuracy :  0.9  loss :  0.27915\n",
      "epoch 1700  accuracy :  0.89  loss :  0.265637\n",
      "epoch 1800  accuracy :  0.93  loss :  0.228631\n",
      "epoch 1900  accuracy :  0.92  loss :  0.33197\n",
      "\n",
      "\n",
      "Test on the test data :  0.9205\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PS : you can use the FileWriter  tf.summary.FileWriter(\"your/path\", sess.graph) \n",
    "to visualize the graph in TensorBoard\n",
    "To access TensorBoard just type the following command in your terminal and go \n",
    "to the localhost:6006 onyour browser: \n",
    "    tensorboard --logdir=\"your/path\" \n",
    "\"\"\"\n",
    "\n",
    "# **********************************\n",
    "#  One Layer Perceptron for MNIST\n",
    "# **********************************\n",
    "\n",
    "\n",
    "\n",
    "# Import the library Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the MNIST data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "\n",
    "\n",
    "# Placeholders for feeding the data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Variables of the model\n",
    "w = tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\n",
    "b = tf.Variable(tf.truncated_normal([10], stddev=0.1)/10)\n",
    "\n",
    "\n",
    "# Use of the 1 layer model \n",
    "y_ = tf.matmul(x,w) + b\n",
    "\n",
    "\n",
    "# Loss (aka cost) and optimization\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# Calculation of accuracy and loss to visualize the training with the print function\n",
    "correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "# Initialization of the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Actual loop for training and updating the variables 1000 times\n",
    "for epoch in range(2000):\n",
    "        \n",
    "        # Splitting of the training set in two variables one with the image and the other one with labels\n",
    "        trainX, trainY = mnist.train.next_batch(100)\n",
    "        # Train\n",
    "        sess.run(train, feed_dict={x : trainX, y : trainY})\n",
    "        \n",
    "        a, l = sess.run([accuracy, cross_entropy], feed_dict={x : trainX, y : trainY})\n",
    "        # I decided to print every 100 iterations to get a global overview of the evolution\n",
    "        if (epoch%100 == 0):\n",
    "            print(\"epoch\", epoch,\" accuracy : \", a, \" loss : \", l)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"\\n\\nTest on the test data : \", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 0  accuracy :  0.39  loss :  5.0687\n",
      "epoch 100  accuracy :  0.97  loss :  0.120728\n",
      "epoch 200  accuracy :  0.98  loss :  0.108522\n",
      "epoch 300  accuracy :  1.0  loss :  0.0575442\n",
      "epoch 400  accuracy :  0.99  loss :  0.094016\n",
      "epoch 500  accuracy :  0.99  loss :  0.0589265\n",
      "epoch 600  accuracy :  1.0  loss :  0.0450771\n",
      "epoch 700  accuracy :  1.0  loss :  0.0570072\n",
      "epoch 800  accuracy :  0.99  loss :  0.0575355\n",
      "epoch 900  accuracy :  0.99  loss :  0.0369172\n",
      "epoch 1000  accuracy :  1.0  loss :  0.0283703\n",
      "epoch 1100  accuracy :  1.0  loss :  0.0609561\n",
      "epoch 1200  accuracy :  0.97  loss :  0.0714867\n",
      "epoch 1300  accuracy :  1.0  loss :  0.0211854\n",
      "epoch 1400  accuracy :  0.99  loss :  0.0811877\n",
      "epoch 1500  accuracy :  1.0  loss :  0.0228478\n",
      "epoch 1600  accuracy :  1.0  loss :  0.0223415\n",
      "epoch 1700  accuracy :  0.99  loss :  0.0372792\n",
      "epoch 1800  accuracy :  1.0  loss :  0.0147056\n",
      "epoch 1900  accuracy :  0.99  loss :  0.0498964\n",
      "epoch 2000  accuracy :  1.0  loss :  0.0134427\n",
      "epoch 2100  accuracy :  1.0  loss :  0.02063\n",
      "epoch 2200  accuracy :  1.0  loss :  0.0206631\n",
      "epoch 2300  accuracy :  1.0  loss :  0.0189762\n",
      "epoch 2400  accuracy :  1.0  loss :  0.0179289\n",
      "epoch 2500  accuracy :  1.0  loss :  0.0296916\n",
      "epoch 2600  accuracy :  0.99  loss :  0.0399791\n",
      "epoch 2700  accuracy :  1.0  loss :  0.0155471\n",
      "epoch 2800  accuracy :  1.0  loss :  0.0159405\n",
      "epoch 2900  accuracy :  1.0  loss :  0.0168992\n",
      "epoch 3000  accuracy :  1.0  loss :  0.014734\n",
      "epoch 3100  accuracy :  1.0  loss :  0.0147545\n",
      "epoch 3200  accuracy :  1.0  loss :  0.0155063\n",
      "epoch 3300  accuracy :  1.0  loss :  0.0173405\n",
      "epoch 3400  accuracy :  1.0  loss :  0.00955427\n",
      "epoch 3500  accuracy :  1.0  loss :  0.0113533\n",
      "epoch 3600  accuracy :  1.0  loss :  0.0150619\n",
      "epoch 3700  accuracy :  1.0  loss :  0.0136547\n",
      "epoch 3800  accuracy :  0.99  loss :  0.014033\n",
      "epoch 3900  accuracy :  1.0  loss :  0.00877074\n",
      "epoch 4000  accuracy :  1.0  loss :  0.00927991\n",
      "epoch 4100  accuracy :  1.0  loss :  0.0122118\n",
      "epoch 4200  accuracy :  1.0  loss :  0.00740885\n",
      "epoch 4300  accuracy :  1.0  loss :  0.00653601\n",
      "epoch 4400  accuracy :  1.0  loss :  0.00528471\n",
      "epoch 4500  accuracy :  1.0  loss :  0.00888222\n",
      "epoch 4600  accuracy :  1.0  loss :  0.00813204\n",
      "epoch 4700  accuracy :  1.0  loss :  0.0123384\n",
      "epoch 4800  accuracy :  1.0  loss :  0.00789369\n",
      "epoch 4900  accuracy :  1.0  loss :  0.00442469\n",
      "\n",
      "\n",
      "Test on the test data :  0.9802\n",
      "\n",
      "\n",
      "We gain a nice .5 in accuracy compare to the 1 layer model.\n",
      "\n",
      "\n",
      "To get even more accuracy a convolutionnal network would be a good way to go.\n",
      "\n",
      "\n",
      "And few twists could be done with the optimizer, learning rate, epochs number and units per layer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PS : you can use the FileWriter  tf.summary.FileWriter(\"your/path\", sess.graph) \n",
    "to visualize the graph in TensorBoard\n",
    "To access TensorBoard just type the following command in your terminal and go \n",
    "to the localhost:6006 onyour browser: \n",
    "    tensorboard --logdir=\"your/path\" \n",
    "\"\"\"\n",
    "\n",
    "# **********************************\n",
    "#  Multi Layer Perceptron for MNIST\n",
    "# **********************************\n",
    "\n",
    "\n",
    "\n",
    "# Import the library Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the MNIST data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "\n",
    "\n",
    "# Placeholders for feeding the data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "# Design of the model\n",
    "def multilayer(x,weights, biases):\n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    layer2 = tf.nn.dropout(layer1, .8)\n",
    "    \n",
    "    out_layer = tf.matmul(layer1,weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Creation of the variables used in the model\n",
    "# Initialization : not using 0 is supposed to make it more easy to converge\n",
    "weights = {\n",
    "    'h1' : tf.Variable(tf.truncated_normal([28*28, 800] ,stddev=0.1)),\n",
    "    #'h2' : tf.Variable(tf.truncated_normal([800, 400] ,stddev=0.1)),\n",
    "    'out' : tf.Variable(tf.truncated_normal([800, 10], stddev=0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.ones([800])/10),\n",
    "    #'b2' : tf.Variable(tf.ones([400])/10),\n",
    "    'out' : tf.Variable(tf.ones([10])/10)\n",
    "}\n",
    "\n",
    "\n",
    "# Use of the multi layer model\n",
    "y_ = multilayer(x, weights, biases)\n",
    "\n",
    "\n",
    "# Loss (aka cost) and optimization\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "#Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialization of the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Actual loop for training and updating the variables 1000 times\n",
    "for epoch in range(5000):\n",
    "        \n",
    "        # Splitting of the training set in two variables one with the image and the other one with labels\n",
    "        trainX, trainY = mnist.train.next_batch(100)\n",
    "        # Training\n",
    "        sess.run(train, feed_dict={x : trainX, y : trainY})\n",
    "        \n",
    "        # accuracy and loss to visualize it on console\n",
    "        a, l = sess.run([accuracy, cross_entropy], feed_dict={x : trainX, y : trainY})\n",
    "        # I decided to print every 100 iterations to get a global overview of the evolution\n",
    "        if (epoch%100 == 0):\n",
    "            print(\"epoch\", epoch,\" accuracy : \", a, \" loss : \", l)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"\\n\\nTest on the test data : \", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "print(\"\\n\\nWe gain a nice .5 in accuracy compare to the 1 layer model.\")\n",
    "print(\"\\n\\nTo get even more accuracy a convolutionnal network would be a good way to go.\")\n",
    "print(\"\\n\\nAnd few twists could be done with the optimizer, learning rate, epochs number and units per \\nlayer.\\\n",
    "adding a dropout to prevent overfitting could be a good idea too\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
