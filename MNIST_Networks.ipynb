{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 0  accuracy :  0.35  loss :  1.94512\n",
      "epoch 100  accuracy :  0.95  loss :  0.284913\n",
      "epoch 200  accuracy :  0.95  loss :  0.273453\n",
      "epoch 300  accuracy :  0.95  loss :  0.214363\n",
      "epoch 400  accuracy :  0.87  loss :  0.490893\n",
      "epoch 500  accuracy :  0.92  loss :  0.270674\n",
      "epoch 600  accuracy :  0.97  loss :  0.196597\n",
      "epoch 700  accuracy :  0.83  loss :  0.476948\n",
      "epoch 800  accuracy :  0.87  loss :  0.324274\n",
      "epoch 900  accuracy :  0.92  loss :  0.270866\n",
      "\n",
      "\n",
      "Test on the test data :  0.9158\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PS : you can use the FileWriter  tf.summary.FileWriter(\"your/path\", sess.graph) \n",
    "to visualize the graph in TensorBoard\n",
    "To access TensorBoard just type the following command in your terminal and go \n",
    "to the localhost:6006 onyour browser: \n",
    "    tensorboard --logdir=\"your/path\" \n",
    "\"\"\"\n",
    "\n",
    "# **********************************\n",
    "#  One Layer Perceptron for MNIST\n",
    "# **********************************\n",
    "\n",
    "\n",
    "\n",
    "# Import the library Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the MNIST data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Placeholders for feeding the data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Variables of the model\n",
    "w = tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\n",
    "b = tf.Variable(tf.truncated_normal([10], stddev=0.1)/10)\n",
    "\n",
    "\n",
    "# Use of the 1 layer model \n",
    "y_ = tf.matmul(x,w) + b\n",
    "\n",
    "\n",
    "# Loss (aka cost) and optimization\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "# Initialization of the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Actual loop for training and updating the variables 1000 times\n",
    "for epoch in range(1000):\n",
    "        \n",
    "        # Splitting of the training set in two variables one with the image and the other one with labels\n",
    "        trainX, trainY = mnist.train.next_batch(100)\n",
    "        # Train\n",
    "        train.run(feed_dict={x : trainX, y : trainY})\n",
    "        # Calculation of accuracy and loss to visualize the training with the print function\n",
    "        correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        a, l = sess.run([accuracy, cross_entropy], feed_dict={x : trainX, y : trainY})\n",
    "        # I decided to print every 100 iterations to get a global overview of the evolution\n",
    "        if (epoch%100 == 0):\n",
    "            print(\"epoch\", epoch,\" accuracy : \", a, \" loss : \", l)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "print(\"\\n\\nTest on the test data : \", accuracy.eval(feed_dict={x:mnist.test.images, y:mnist.test.labels}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 0  accuracy :  0.29  loss :  5.61723\n",
      "epoch 100  accuracy :  0.98  loss :  0.12464\n",
      "epoch 200  accuracy :  0.98  loss :  0.107897\n",
      "epoch 300  accuracy :  1.0  loss :  0.0576248\n",
      "epoch 400  accuracy :  0.98  loss :  0.108382\n",
      "epoch 500  accuracy :  0.99  loss :  0.0563672\n",
      "epoch 600  accuracy :  0.99  loss :  0.0716578\n",
      "epoch 700  accuracy :  0.97  loss :  0.0829052\n",
      "epoch 800  accuracy :  0.99  loss :  0.0472509\n",
      "epoch 900  accuracy :  0.99  loss :  0.0482935\n",
      "\n",
      "\n",
      "Test on the test data :  0.9666\n",
      "\n",
      "\n",
      "We gain almost .4 in accuracy compare to the 1 layer model.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PS : you can use the FileWriter  tf.summary.FileWriter(\"your/path\", sess.graph) \n",
    "to visualize the graph in TensorBoard\n",
    "To access TensorBoard just type the following command in your terminal and go \n",
    "to the localhost:6006 onyour browser: \n",
    "    tensorboard --logdir=\"your/path\" \n",
    "\"\"\"\n",
    "\n",
    "# **********************************\n",
    "#  Multi Layer Perceptron for MNIST\n",
    "# **********************************\n",
    "\n",
    "\n",
    "\n",
    "# Import the library Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the MNIST data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Placeholders for feeding the data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "# Design of the model\n",
    "def multilayer(x,weights, biases):\n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    layer2 = tf.add(tf.matmul(layer1,weights['h2']), biases['b2'])\n",
    "    layer2 = tf.nn.relu(layer1)\n",
    "    \n",
    "    out_layer = tf.matmul(layer2,weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Creation of the variables used in the model\n",
    "# Initialization : not using 0 is supposed to make it more easy to converge\n",
    "weights = {\n",
    "    'h1' : tf.Variable(tf.truncated_normal([784, 784], stddev=0.1)),\n",
    "    'h2' : tf.Variable(tf.truncated_normal([784, 784], stddev=0.1)),\n",
    "    'out' : tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.ones([784])/10),\n",
    "    'b2' : tf.Variable(tf.ones([784])/10),\n",
    "    'out' : tf.Variable(tf.ones([10])/10)\n",
    "}\n",
    "\n",
    "\n",
    "# Use of the multi layer model\n",
    "y_ = multilayer(x, weights, biases)\n",
    "\n",
    "\n",
    "# Loss (aka cost) and optimization\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "# Initialization of the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Actual loop for training and updating the variables 1000 times\n",
    "for epoch in range(1000):\n",
    "        \n",
    "        # Splitting of the training set in two variables one with the image and the other one with labels\n",
    "        trainX, trainY = mnist.train.next_batch(100)\n",
    "        # Training\n",
    "        train.run(feed_dict={x : trainX, y : trainY})\n",
    "        # Calculation of accuracy and loss to visualize the training with the print function\n",
    "        correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        a, l = sess.run([accuracy, cross_entropy], feed_dict={x : trainX, y : trainY})\n",
    "        # I decided to print every 100 iterations to get a global overview of the evolution\n",
    "        if (epoch%100 == 0):\n",
    "            print(\"epoch\", epoch,\" accuracy : \", a, \" loss : \", l)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "print(\"\\n\\nTest on the test data : \", accuracy.eval(feed_dict={x:mnist.test.images, y:mnist.test.labels}))\n",
    "print(\"\\n\\nWe gain a nice .5 in accuracy compare to the 1 layer model.\")\n",
    "print(\"\\n\\nTo get even more accuracy a convolutionnal network would be a good way to go.\")\n",
    "print(\"\\n\\nAnd few twists could be done with the optimizer and the learning rate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
