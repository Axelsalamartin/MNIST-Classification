{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 0  accuracy :  0.33  loss :  1.90097\n",
      "epoch 100  accuracy :  0.96  loss :  0.274884\n",
      "epoch 200  accuracy :  0.95  loss :  0.276589\n",
      "epoch 300  accuracy :  0.96  loss :  0.219923\n",
      "epoch 400  accuracy :  0.89  loss :  0.474325\n",
      "epoch 500  accuracy :  0.92  loss :  0.263503\n",
      "epoch 600  accuracy :  0.93  loss :  0.281929\n",
      "epoch 700  accuracy :  0.91  loss :  0.23556\n",
      "epoch 800  accuracy :  0.94  loss :  0.226706\n",
      "epoch 900  accuracy :  0.9  loss :  0.361896\n",
      "epoch 1000  accuracy :  0.92  loss :  0.305134\n",
      "epoch 1100  accuracy :  0.93  loss :  0.276063\n",
      "epoch 1200  accuracy :  0.94  loss :  0.208141\n",
      "epoch 1300  accuracy :  0.92  loss :  0.361163\n",
      "epoch 1400  accuracy :  0.93  loss :  0.294054\n",
      "epoch 1500  accuracy :  0.91  loss :  0.271535\n",
      "epoch 1600  accuracy :  0.93  loss :  0.264699\n",
      "epoch 1700  accuracy :  0.95  loss :  0.207738\n",
      "epoch 1800  accuracy :  0.88  loss :  0.379158\n",
      "epoch 1900  accuracy :  0.87  loss :  0.446189\n",
      "\n",
      "\n",
      "Test on the test data :  0.9206\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PS : you can use the FileWriter  tf.summary.FileWriter(\"your/path\", sess.graph) \n",
    "to visualize the graph in TensorBoard\n",
    "To access TensorBoard just type the following command in your terminal and go \n",
    "to the localhost:6006 onyour browser: \n",
    "    tensorboard --logdir=\"your/path\" \n",
    "\"\"\"\n",
    "\n",
    "# **********************************\n",
    "#  One Layer Perceptron for MNIST\n",
    "# **********************************\n",
    "\n",
    "\n",
    "\n",
    "# Import the library Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the MNIST data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "\n",
    "\n",
    "# Placeholders for feeding the data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Variables of the model\n",
    "w = tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\n",
    "b = tf.Variable(tf.truncated_normal([10], stddev=0.1)/10)\n",
    "\n",
    "\n",
    "# Use of the 1 layer model \n",
    "y_ = tf.matmul(x,w) + b\n",
    "\n",
    "\n",
    "# Loss (aka cost) and optimization\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# Calculation of accuracy and loss to visualize the training with the print function\n",
    "correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "# Initialization of the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Actual loop for training and updating the variables 1000 times\n",
    "for epoch in range(2000):\n",
    "        \n",
    "        # Splitting of the training set in two variables one with the image and the other one with labels\n",
    "        trainX, trainY = mnist.train.next_batch(100)\n",
    "        # Train\n",
    "        sess.run(train, feed_dict={x : trainX, y : trainY})\n",
    "        \n",
    "        a, l = sess.run([accuracy, cross_entropy], feed_dict={x : trainX, y : trainY})\n",
    "        # I decided to print every 100 iterations to get a global overview of the evolution\n",
    "        if (epoch%100 == 0):\n",
    "            print(\"epoch\", epoch,\" accuracy : \", a, \" loss : \", l)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"\\n\\nTest on the test data : \", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 0  accuracy :  0.24  loss :  2.24391\n",
      "epoch 100  accuracy :  0.98  loss :  0.167223\n",
      "epoch 200  accuracy :  0.98  loss :  0.10839\n",
      "epoch 300  accuracy :  0.99  loss :  0.0705742\n",
      "epoch 400  accuracy :  0.97  loss :  0.0993604\n",
      "epoch 500  accuracy :  0.99  loss :  0.050265\n",
      "epoch 600  accuracy :  0.99  loss :  0.0564823\n",
      "epoch 700  accuracy :  0.99  loss :  0.0412677\n",
      "epoch 800  accuracy :  0.99  loss :  0.0553989\n",
      "epoch 900  accuracy :  0.99  loss :  0.0750023\n",
      "epoch 1000  accuracy :  0.98  loss :  0.102572\n",
      "epoch 1100  accuracy :  1.0  loss :  0.0255843\n",
      "epoch 1200  accuracy :  1.0  loss :  0.0211614\n",
      "epoch 1300  accuracy :  1.0  loss :  0.0129302\n",
      "epoch 1400  accuracy :  0.99  loss :  0.0279897\n",
      "epoch 1500  accuracy :  1.0  loss :  0.0139054\n",
      "epoch 1600  accuracy :  0.99  loss :  0.0454767\n",
      "epoch 1700  accuracy :  1.0  loss :  0.0122034\n",
      "epoch 1800  accuracy :  1.0  loss :  0.00862625\n",
      "epoch 1900  accuracy :  1.0  loss :  0.0160454\n",
      "\n",
      "\n",
      "Test on the test data :  0.9727\n",
      "\n",
      "\n",
      "We gain a nice .5 in accuracy compare to the 1 layer model.\n",
      "\n",
      "\n",
      "To get even more accuracy a convolutionnal network would be a good way to go.\n",
      "\n",
      "\n",
      "And few twists could be done with the optimizer and the learning rate.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PS : you can use the FileWriter  tf.summary.FileWriter(\"your/path\", sess.graph) \n",
    "to visualize the graph in TensorBoard\n",
    "To access TensorBoard just type the following command in your terminal and go \n",
    "to the localhost:6006 onyour browser: \n",
    "    tensorboard --logdir=\"your/path\" \n",
    "\"\"\"\n",
    "\n",
    "# **********************************\n",
    "#  Multi Layer Perceptron for MNIST\n",
    "# **********************************\n",
    "\n",
    "\n",
    "\n",
    "# Import the library Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the MNIST data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n",
    "\n",
    "\n",
    "# Placeholders for feeding the data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "\n",
    "# Design of the model\n",
    "def multilayer(x,weights, biases):\n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    layer2 = tf.add(tf.matmul(layer1,weights['h2']), biases['b2'])\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer2,weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Creation of the variables used in the model\n",
    "# Initialization : not using 0 is supposed to make it more easy to converge\n",
    "weights = {\n",
    "    'h1' : tf.Variable(tf.truncated_normal([28*28, 200] ,stddev=0.1)),\n",
    "    'h2' : tf.Variable(tf.truncated_normal([200, 100] ,stddev=0.1)),\n",
    "    'out' : tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.ones([200])/10),\n",
    "    'b2' : tf.Variable(tf.ones([100])/10),\n",
    "    'out' : tf.Variable(tf.ones([10])/10)\n",
    "}\n",
    "\n",
    "\n",
    "# Use of the multi layer model\n",
    "y_ = multilayer(x, weights, biases)\n",
    "\n",
    "\n",
    "# Loss (aka cost) and optimization\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "#Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialization of the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Actual loop for training and updating the variables 1000 times\n",
    "for epoch in range(2000):\n",
    "        \n",
    "        # Splitting of the training set in two variables one with the image and the other one with labels\n",
    "        trainX, trainY = mnist.train.next_batch(100)\n",
    "        # Training\n",
    "        sess.run(train, feed_dict={x : trainX, y : trainY})\n",
    "        \n",
    "        # accuracy and loss to visualize it on console\n",
    "        a, l = sess.run([accuracy, cross_entropy], feed_dict={x : trainX, y : trainY})\n",
    "        # I decided to print every 100 iterations to get a global overview of the evolution\n",
    "        if (epoch%100 == 0):\n",
    "            print(\"epoch\", epoch,\" accuracy : \", a, \" loss : \", l)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"\\n\\nTest on the test data : \", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "print(\"\\n\\nWe gain a nice .5 in accuracy compare to the 1 layer model.\")\n",
    "print(\"\\n\\nTo get even more accuracy a convolutionnal network would be a good way to go.\")\n",
    "print(\"\\n\\nAnd few twists could be done with the optimizer and the learning rate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
